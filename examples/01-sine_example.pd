#N canvas 694 166 1353 935 10;
#X obj 47 683 neuralnet;
#X floatatom 98 707 10 0 0 1 loss - - 0;
#X floatatom 87 730 8 0 0 1 accuracy - - 0;
#X floatatom 67 753 8 0 0 1 epoch - - 0;
#X obj 47 827 s predictions;
#X msg 926 751 destroy;
#X msg 47 489 data_out_arrays output_data;
#X msg 47 417 data_in_arrays input_data;
#X msg 925 130 train;
#X text 47 89 1) create a network;
#N canvas 388 317 839 517 data_arrays 0;
#X obj 31 183 array size input_data;
#X obj 31 229 until;
#X obj 31 252 f;
#X obj 31 276 t f f;
#X obj 31 127 loadbang;
#X obj 58 252 + 1;
#X obj 31 206 t f f;
#X obj 31 362 pack, f 5;
#X obj 88 317 /;
#X obj 31 434 array define input_data 1000;
#X obj 215 434 array define output_data 1000;
#X obj 31 299 t f f;
#X obj 88 344 t f f;
#X obj 31 331 t f f;
#X obj 236 362 *;
#N canvas 1280 491 450 300 two_pi 0;
#X msg 136 117 1;
#X obj 136 139 atan;
#X obj 136 93 loadbang;
#X obj 136 183 outlet;
#X obj 136 161 * 8;
#X connect 0 0 1 0;
#X connect 1 0 4 0;
#X connect 2 0 0 0;
#X connect 4 0 3 0;
#X restore 251 338 pd two_pi;
#X obj 215 410 pack;
#X obj 236 385 sin;
#X obj 411 229 until;
#X obj 411 252 f;
#X obj 411 276 t f f;
#X obj 438 252 + 1;
#X obj 411 206 t f f;
#X obj 411 359 pack, f 5;
#X obj 478 317 /;
#X obj 411 299 t f f;
#X obj 31 155 t b b;
#X obj 636 359 *;
#N canvas 1280 491 450 300 two_pi 0;
#X msg 136 117 1;
#X obj 136 139 atan;
#X obj 136 93 loadbang;
#X obj 136 183 outlet;
#X obj 136 161 * 8;
#X connect 0 0 1 0;
#X connect 1 0 4 0;
#X connect 2 0 0 0;
#X connect 4 0 3 0;
#X restore 651 335 pd two_pi;
#X obj 615 407 pack;
#X obj 636 382 sin;
#X obj 478 341 t f f;
#X obj 411 329 t f f;
#X text 29 13 the two pairs of arrays in this subpatch create the input and output for the training and testing data sets for the neural network \; the input is a ramp from 0 to 1 and the output is a sine wave. the training data set consists of 1000 elements and the testing data set of 100 elements;
#X obj 411 173 array size input_validate;
#X obj 411 382 array define input_validate 100;
#X obj 615 431 array define output_validate 100;
#X connect 0 0 6 0;
#X connect 1 0 2 0;
#X connect 2 0 3 0;
#X connect 3 0 11 0;
#X connect 3 1 5 0;
#X connect 4 0 26 0;
#X connect 5 0 2 1;
#X connect 6 0 1 0;
#X connect 6 1 8 1;
#X connect 7 0 9 0;
#X connect 8 0 12 0;
#X connect 11 0 13 0;
#X connect 11 1 8 0;
#X connect 12 0 7 1;
#X connect 12 1 14 0;
#X connect 13 0 7 0;
#X connect 13 1 16 0;
#X connect 14 0 17 0;
#X connect 15 0 14 1;
#X connect 16 0 10 0;
#X connect 17 0 16 1;
#X connect 18 0 19 0;
#X connect 19 0 20 0;
#X connect 20 0 25 0;
#X connect 20 1 21 0;
#X connect 21 0 19 1;
#X connect 22 0 18 0;
#X connect 22 1 24 1;
#X connect 23 0 35 0;
#X connect 24 0 31 0;
#X connect 25 0 32 0;
#X connect 25 1 24 0;
#X connect 26 0 0 0;
#X connect 26 1 34 0;
#X connect 27 0 30 0;
#X connect 28 0 27 1;
#X connect 29 0 36 0;
#X connect 30 0 29 1;
#X connect 31 0 23 1;
#X connect 31 1 27 0;
#X connect 32 0 23 0;
#X connect 32 1 29 0;
#X connect 34 0 22 0;
#X restore 926 842 pd data_arrays;
#N canvas 0 50 450 250 (subpatch) 0;
#X array ai_sine 1000 float 0;
#X coords 0 1 999 -1 140 100 2 0 0;
#X restore 215 659 graph;
#X text 45 160 2) set activation functions if different that default;
#X obj 925 540 r predictions;
#X obj 928 469 hsl 128 15 0 1 0 0 empty empty empty -2 -8 0 10 #fcfcfc #000000 #000000 0 1;
#X msg 925 489 predict \$1;
#X obj 925 566 vsl 15 128 -1 1 0 0 empty empty empty 0 -9 0 10 #fcfcfc #000000 #000000 0 1;
#X msg 47 182 set_activation_function 0 relu \, set_activation_function 1 relu \, set_activation_function 2 linear;
#X msg 47 266 set_loss_function mse;
#X text 46 243 3) set the loss function;
#X msg 47 337 set_optimizer adam;
#X text 46 315 4) set the optimizer;
#X text 47 390 5) set input data array (found in [pd data_arrays]);
#X text 46 465 6) set output data array (found in [pd data_arrays]);
#X msg 925 370 validate;
#X msg 47 111 create 1 64 64 1 \, regression;
#X text 1021 842 <- training and validating data set arrays;
#X msg 925 211 data_in_arrays input_validate;
#X msg 925 285 data_out_arrays output_validate;
#X obj 926 774 s sine_example;
#X obj 47 134 s sine_example;
#X obj 47 218 s sine_example;
#X obj 47 289 s sine_example;
#X obj 47 360 s sine_example;
#X obj 47 440 s sine_example;
#X obj 47 512 s sine_example;
#X obj 925 153 s sine_example;
#X obj 925 234 s sine_example;
#X obj 925 308 s sine_example;
#X obj 925 393 s sine_example;
#X obj 925 512 s sine_example;
#X obj 47 659 r sine_example;
#X msg 472 356 store_weights_during_training 0 0 weights;
#X obj 472 379 s sine_example;
#X floatatom 472 443 5 0 0 0 - - - 0;
#X msg 472 465 weights_scale_coeff 0 \$1;
#X obj 472 488 s sine_example;
#N canvas 0 50 450 250 (subpatch) 0;
#X array weights 64 float 0;
#X coords 0 1 63 -1 140 100 1 0 0;
#X restore 470 730 graph;
#X text 470 97 8) if you want to visualize the training process \, you can send the store_weights_during_training or store_biases_during_training messages \, so you can store weights or biases to an array;
#X text 469 159 store_weights_during_training takes three arguments \, the layer number (0 being the first hidden layer \, as the input layer has no weights) \, the input number to that layer \, and the array name (which MUST have the same size as the outputs of that layer);
#X text 469 239 store_biases_during_training takes two arguments \, the layer number (0 being the first hidden layer \, as the input layer has no biases) \, and the array name (which MUST have the same size as the outputs of that layer);
#X text 469 315 store the weights of the first layer and its first input to array named "weights";
#X text 469 410 set a scaling coefficient \, if you like. arguments are layer number and scaling coefficient;
#N canvas 0 50 450 250 (subpatch) 0;
#X array biases 64 float 0;
#X coords 0 1 63 -1 140 100 1 0 0;
#X restore 630 730 graph;
#X obj 472 566 s sine_example;
#X floatatom 472 634 5 0 0 0 - - - 0;
#X obj 472 679 s sine_example;
#X text 469 601 set a scaling coefficient \, if you like. arguments are layer number and scaling coefficient;
#X msg 472 543 store_biases_during_training 0 biases;
#X msg 472 656 biases_scale_coeff 0 \$1;
#X text 470 838 these two arrays have size 64 \, the number of outputs of the first hidden layer;
#X text 922 433 13) or test the network manually and check its output in the vertical slider below;
#X text 924 713 14) when done \, destroy the network (it will also be destroyed if you just close this patch);
#X msg 46 586 output_during_training 1;
#X obj 93 803 s ai_sine;
#X obj 47 777 route float list;
#X obj 46 609 s sine_example;
#X text 46 537 7) if you want to visualize the development of the network's predictions \, click the message below and see the ai_sine array;
#X text 924 188 10) set a test input data array (found in [pd data_arrays]);
#X text 922 261 11) set a test target data array (found in [pd data_arrays]);
#X text 923 106 9) train the model;
#X text 923 335 12) validate the model (check the number atoms connected to the outlets of [neuralnet]);
#X text 469 519 store the biases of the first layer to array named "biases";
#X text 47 20 In this example \, a neural network is trying to "learn" the sine function \, by receiving input from 0 to 1;
#X msg 975 130 abort_training;
#X connect 0 0 65 0;
#X connect 0 2 3 0;
#X connect 0 4 2 0;
#X connect 0 5 1 0;
#X connect 5 0 29 0;
#X connect 6 0 35 0;
#X connect 7 0 34 0;
#X connect 8 0 36 0;
#X connect 13 0 16 0;
#X connect 14 0 15 0;
#X connect 15 0 40 0;
#X connect 17 0 31 0;
#X connect 18 0 32 0;
#X connect 20 0 33 0;
#X connect 24 0 39 0;
#X connect 25 0 30 0;
#X connect 27 0 37 0;
#X connect 28 0 38 0;
#X connect 41 0 0 0;
#X connect 42 0 43 0;
#X connect 44 0 45 0;
#X connect 45 0 46 0;
#X connect 55 0 59 0;
#X connect 58 0 54 0;
#X connect 59 0 56 0;
#X connect 63 0 66 0;
#X connect 65 0 4 0;
#X connect 65 1 64 0;
#X connect 74 0 36 0;
