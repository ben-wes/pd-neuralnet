#N canvas 436 72 892 1070 10;
#X obj 33 9 cnv 15 380 120 empty empty empty 20 12 0 14 #c4fcc4 #404040 0;
#X text 38 106 Written by Alexandros Drymonitis;
#N canvas 284 184 660 668 loss_function_messages 0;
#X msg 44 51 set_loss_function \$1;
#X text 183 50 Set the loss function of the network. Argument:;
#X text 183 77 Symbol with name of loss function. Can be either of the following:;
#X text 182 116 "mse": Mean Squared Error;
#X text 182 136 "mae": Mean Absolute Error;
#X text 182 156 "cat_x_entropy": Categorical Cross-Entropy;
#X text 186 200 Defaults to "mse";
#X msg 44 250 set_weight_reg1 \$1 \$2;
#X msg 44 350 set_weight_reg2 \$1 \$2;
#X msg 44 450 set_bias_reg1 \$1 \$2;
#X msg 44 550 set_bias_reg2 \$1 \$2;
#X text 193 247 Set the weight regularizer 1 value of a layer (used for optimizing loss function). Arguments:;
#X text 193 283 \$1: layer number;
#X text 193 301 \$2: weight regularizer 1 (defaults to 0);
#X text 191 349 Set the weight regularizer 2 value of a layer (used for optimizing loss function). Arguments:;
#X text 193 386 \$1: layer number;
#X text 193 404 \$2: weight regularizer 2 (defaults to 0);
#X text 191 449 Set the bias regularizer 1 value of a layer (used for optimizing loss function). Arguments:;
#X text 193 487 \$1: layer number;
#X text 193 505 \$2: bias regularizer 1 (defaults to 0);
#X text 191 549 Set the bias regularizer 2 value of a layer (used for optimizing loss function). Arguments:;
#X text 193 591 \$1: layer number;
#X text 193 609 \$2: bias regularizer 2 (defaults to 0);
#X text 182 176 "bin_x_entropy": Binary Cross-Entropy;
#X restore 523 137 pd loss_function_messages;
#N canvas 385 317 670 544 optimizer_messages 0;
#X msg 29 34 set_optimizer \$1;
#X text 144 36 Set the optimizer of the network. Argument:;
#X text 144 61 \$1: Symbol with optimizer name. It can be either of the following:;
#X text 144 99 "adam": Adam optimizer;
#X text 144 119 "sgd": SGD optimizer;
#X text 144 139 "adagrad": Adagrad optimizer;
#X text 144 159 "rms_prop": RMSprop optimizer;
#X text 147 186 Defaults to "adam";
#X msg 38 261 set_learning_rate \$1;
#X text 171 260 Set the learning rate of the optimizer. Dafaults to 0.005;
#X msg 38 291 set_decay \$1;
#X msg 38 321 set_beta1 \$1;
#X msg 38 361 set_beta2 \$1;
#X msg 38 401 set_epsilon \$1;
#X msg 38 441 set_rho \$1;
#X text 171 290 Set the decay of the optimizer. Dafaults to 0.001;
#X msg 38 471 set_momentum \$1;
#X text 171 470 Set the momentum variable of the SGD optimizer. Dafaults to 0.9;
#X text 171 440 Set the rho variable of the RMSProp optimizer. Dafaults to 0.9;
#X text 171 400 Set the epsilon variable of the optimizer (used by all optimizers except from SGD). Dafaults to 1e-07;
#X text 171 320 Set the beta1 variable of the Adam optimizer. Dafaults to 0.9;
#X text 171 360 Set the beta2 variable of the Adam optimizer. Dafaults to 0.999;
#X restore 523 165 pd optimizer_messages;
#N canvas 863 449 512 289 saving_and_loading_models_messages 0;
#X msg 27 24 save \$1;
#X text 87 24 Save a model. Argument:;
#X text 87 43 \$1: /path/to/model/name.ann (default file extension is .ann);
#X text 87 173 \$1: /path/to/model/name.ann (default file extension is .ann);
#X msg 27 154 load \$1;
#X text 87 154 Load a model. Argument:;
#X text 91 62 Relative paths work too. The file extension is not necessary \, if absent \, it is added by the object;
#X text 91 194 Relative paths work too. The file extension is not necessary \, if absent \, it is added by the object;
#X restore 523 193 pd saving_and_loading_models_messages;
#X text 36 151 Arguments (optional):;
#X text 36 703 Inlet: various messages (check the subpatches to the right);
#X text 37 737 Outlets:;
#X text 35 759 1: predictions;
#X text 35 798 3: epoch count (outputs during training);
#N canvas 429 144 1152 727 training_validating_and_predicting 0;
#X msg 29 191 train [\$1];
#X text 107 190 Train the network. Argument (optional):;
#X text 145 378 Give a prediction based on the input of the arguments. Arguments:;
#X text 147 415 \$1: first input (N);
#X text 147 432 $<N+1>: N+1 input;
#X text 150 451 ...;
#X text 147 477 The number of arguments must be equal to the number of inputs of the network (the first argument of the network creation message);
#X msg 612 31 pause_training;
#X text 707 31 Pauses a training session;
#X msg 612 61 resume_training;
#X text 713 61 Resumes a training session;
#X msg 612 91 abort_training;
#X text 710 91 Aborts a training session ("resume_training" is ineffective after this message);
#X text 106 210 percentage of the training data set that should be used for validating (e.g. if argument is 10 \, then 10% of the training data set will be used for validating \, in which case no further data should be imported after training and before testing). Note \, if batch training \, this percentage value MUST be passed via the "set_batch_size" message and not via "train"!;
#X msg 33 329 validate;
#X text 94 329 Validate the network;
#X msg 32 379 predict \$1 ...;
#X msg 29 31 set_epochs \$1;
#X text 125 32 Set the number of epochs for training;
#X msg 29 69 set_batch_size \$1 [\$2];
#X text 175 66 Set the number of samples in a batch (used when training in batches). In case a percentage of the training data should be used for validation \, the second argument is that percentage (e.g. 10 for 10% of the data). Note that this extra argument can be passed through the "train" message \, but in case of batch training \, it MUST be passed through the "set_batch_size" message!;
#X msg 612 141 set_train_del \$1;
#X text 720 141 Set the delay in milliseconds between training epochs (used to not freeze Pd while training). Defaults to 25;
#X msg 608 226 desired_loss \$1;
#X msg 608 336 desired_accuracy \$1;
#X msg 608 436 retrain;
#X msg 608 598 release_mem;
#X text 692 594 Release the training memory after a training session is over \, while the loss or accuracy of the network have not met the requirements of "desired_loss" and "desired_accuracy" respectively;
#X msg 608 506 keep_traiing;
#X text 664 436 Retrains the network when the "desired_loss" or "desired_accuracy" messages have been sent \, resetting weights and biases to their initial values (use this and not "train");
#X text 694 506 Retrains the network when the "desired_loss" or "desired_accuracy" messages have been sent \, without resetting weights and biases \, so the network can be trained for more epochs than what has been set via "set_epochs" (use this and not "train");
#X text 715 225 Set a loss threshold \, where \, if the loss of the network is above it after training is done \, the training dataset memory will not be cleared and freed \, so you can change some parameters of the network and retrain it without having to reload the training dataset. Defaults to FLT_MAX (3.40282e+38);
#X text 745 335 Set an accuracy threshold \, where \, if the accuracy of the network is below it after training is done \, the training dataset memory will not be cleared and freed \, so you can change some parameters of the network and retrain it without having to reload the training dataset. Defaults to 0;
#X restore 523 81 pd training_validating_and_predicting;
#N canvas 170 106 1200 793 dataset_messages 0;
#X text 176 123 ...;
#X text 160 547 ...;
#X text 155 488 \$1: first input datum (N);
#X text 175 78 \$1: first input data array (N);
#X text 175 98 $<N+1>: second input data array;
#X text 155 509 $<N+1>: N+1 input datum (in case network has more than one inputs);
#X text 160 607 ...;
#X text 156 633 The total number of arguments must equal the number of input and output neurons of the network (the first and last argument of the network creation message);
#X text 773 334 \$1: first input datum maximum value (N);
#X text 773 354 $<N+1>: N+1 input datum maximum value;
#X text 775 388 ...;
#X text 776 407 The number of arguments should be equal to the inputs of the network (the first argument of the network creation message);
#X text 776 612 ...;
#X text 774 569 \$1: first output datum maximum value (N);
#X text 774 589 $<N+1>: N+1 output datum maximum value;
#X text 779 642 The number of arguments should be equal to the outputs of the network (the last argument of the network creation message);
#X msg 623 711 shuffle_train_set;
#X text 740 709 Shuffle the training data set (used for better fitting the network);
#X msg 34 28 data_in_arrays \$1 ...;
#X msg 34 268 data_out_arrays \$1 ...;
#X msg 616 275 normalize_input \$1 ...;
#X msg 616 515 normalize_output \$1 ...;
#X msg 622 27 add_arrays \$1 \$2;
#X text 732 160 \$1: Array name for the input layer. The size of the array must be equal to the number of input neurons of the network;
#X text 732 197 \$2: Array name for the output layer. The size of the array must be equal to the number of output neurons of the network;
#X text 154 575 $<N+num_in_data>: first output datum;
#X text 174 148 In case training or validating data are set via arrays in Pd \, the names of the arrays have to be set with this message. The number of arguments must be equal to the first argument in the network creation message (the number of inputs of the network) and the size of all arrays must be equal;
#X text 182 264 Similar to the "data_in_arrays" message \, only for output training or validating data. This message must be sent after the "data_in_arrays" message. The number of arguments must be equal to the last argument in the network creation message (the number of output neurons of the network) and the size of all arrays must be equal;
#X msg 35 394 add [\$1 \$2] ...;
#X text 156 393 Add training or testing data manually. If arrays are added with the "add_arrays" message (look to the right column in this subpatch) \, then "add" takes no arguments. Otherwise it takes at least two arguments (for a network with single input and a single output).;
#X text 156 469 Arguments (optional):;
#X text 173 27 Set the names of the arrays that hold the input training data.;
#X text 174 60 Arguments:;
#X text 732 25 This message adds arrays vertically for the input and output of the network (meaning \, each element of an array corresponds to one neuron in the input or output layer). This is usefull when all training or predicting data is loaded to an array in Pd \, for every sample separately. Once this message is sent \, every time new data is imported \, the message "add" with no arguments must be sent \, and [neuralnet] will read the data out of these two arrays;
#X text 733 141 Arguments:;
#X text 770 274 Maximum values of the inputs \, used to normalize data to a range from 0 to 1 or from -1 to 1;
#X text 771 312 Arguments:;
#X text 774 514 Maximum values of the outputs \, used to normalize data to a range from 0 to 1 or from -1 to 1 .;
#X text 774 550 Arguments:;
#X restore 523 53 pd dataset_messages;
#N canvas 286 373 1393 762 layer_messages 0;
#X text 242 335 "linear": Linear activation function;
#X text 242 355 "sigmoid": Sigmoid activation function;
#X text 242 397 "relu": ReLU activation function;
#X text 242 479 "softmax": Softmax activation function;
#X text 246 502 Defaults to "sigmoid";
#X msg 44 241 set_activation_function \$1 \$2;
#X text 243 296 \$2: Symbol with name of activation function. Can be either of the following:;
#X text 243 240 Set the activation function for a layer. Arguments:;
#X text 244 268 \$1: Layer number \, starting from 0;
#X msg 39 164 set_weight_coeff \$1;
#X text 167 160 Set the scale coefficient of the weights for all layers. Defaults to 0.1;
#X msg 39 54 set_dropout \$1 \$2;
#X text 167 50 Set the dropout rate for the input of a layer. Arguments:;
#X text 170 71 \$1: Layer index (starting from 0 \, cannot be 0 as input layer can't use a dropout in its input);
#X msg 44 600 seed \$1;
#X text 102 596 Set the seed for the rand() function used to initialize the weights of the layers. Internally \, the seed is set on object initialization with srand(time(0)) \, but some systems may not have a reliable time (e.g. a Raspberry Pi that is not constantly powered) \, so "seed" enables setting the random seed externally;
#X text 170 106 \$2: Dropout rate (e.g. 0.1 means 10% of neuros are dropped in each epoch);
#X text 242 376 "bipolar_sigmoid": Bipolar Sigmoid activation function;
#X text 242 417 "leaky_relu": Leaky ReLU activation function;
#X text 242 438 "rect_softplus": Rectified Softplus activation function;
#X text 242 457 "tanh": Hyperbolic Tangent activation function;
#X msg 632 56 store_weights_during_training \$1 \$2 \$3;
#X text 876 53 Store the weights during the training process;
#X text 883 75 \$1: layer index \, first hidden layer has index 0;
#X text 883 90 \$2: layer neuron index;
#X text 884 106 \$3: array name to write the weights to;
#X text 876 273 Store the weights during the training process;
#X text 883 295 \$1: layer index \, first hidden layer has index 0;
#X msg 632 276 store_biases_during_training \$1 \$2;
#X text 884 315 \$2: array name to write the weights to;
#X msg 632 155 weights_scale_coeff \$1 \$2;
#X text 800 154 Scaling coefficient for stored weights \, in case these exceed the array boundaries;
#X text 817 186 \$1: layer index \, first hidden layer has index 0;
#X text 817 202 \$2: scaling coefficient;
#X text 817 416 \$1: layer index \, first hidden layer has index 0;
#X text 817 432 \$2: scaling coefficient;
#X msg 632 385 biases_scale_coeff \$1 \$2;
#X text 800 384 Scaling coefficient for stored biases \, in case these exceed the array boundaries;
#X text 818 465 See example 1 for these four features in practice;
#X restore 523 109 pd layer_messages;
#N canvas 280 172 1313 766 various_netwrok_messages 0;
#X text 186 43 create a network. arguements:;
#X text 186 60 \$1: number of neurons in input layer;
#X text 186 76 \$2: number of neurons in first hidden layer (N);
#X text 186 92 \$3: if this is the last argument it is the nubmer of neurons in the output layer \, otherwise it is the number of neurons in N+1 hidden layer;
#X text 186 142 ...;
#X text 186 170 $<N+2>: number of neurons in the output layer (unless there are only three arguments \, in any case \, the last argument is sets the output layer);
#X msg 50 440 destroy;
#X text 105 440 destroy the network;
#X text 184 228 for example \, the message "create 2 64 64 3" creates a network with two inputs \, two hidden layers with 64 neurons each \, and three output neurons.;
#X msg 51 43 create \$1 \$2 \$3 ...;
#X msg 51 311 classification;
#X text 151 309 Set the network to classification mode;
#X msg 51 341 regression;
#X text 124 339 Set the network to regression mode;
#X msg 51 371 binary_logistic_regression;
#X text 221 369 Set the network to binary logistic regression mode;
#X msg 679 31 predict_from \$1;
#X msg 679 91 predict_to \$1;
#X text 787 32 Set an array to draw input for prediction from. Argument: Array name that holds input for the network (use "inlet" to go back to receiving input from the inlet of the object);
#X text 787 92 Set an array to write predictions to. Argument: Array name to write the network's output to (use "outlet" to go back to outputting predictions from the object's first outlet);
#X msg 679 161 morph \$1 \$1;
#X text 768 195 \$1: /path/to/saved/model.ann to morph to;
#X text 768 213 \$2: ramp time in milliseconds to ramp to new model;
#X text 767 162 Morph between to saved models. Adapted from [line]'s source code. Arguments:;
#X msg 679 273 confidences \$1;
#X text 779 270 Set whether the network should output a list of confidences instead of a predicted class (used only in classification mode) (0|1);
#X msg 679 354 confidence_thresh \$1;
#X text 813 352 Set a confidence threshold below which the network will output a list with class prediction and its confidence out the second outlet. This is effective only when the "confidences" message is 0 (the network outputs a predicted class \, and not a list of confidences). Argument:;
#X text 812 427 \$1: value from 0 to 1 (1 not recommended \, the netwrok will never output anything). Defaults to 0 (disabled);
#X msg 679 522 set_accuracy_denominator \$1;
#X text 869 519 Set the denominator to determine the accuracy threshold when in regression mode. This is defined by the standard deviation of the output training data \, divided by this number. Defaults to 250;
#X msg 50 506 output_during_training \$1;
#X text 222 533 \$1: 0-1 to (de)activate this feature;
#X text 215 501 Send predictions out the outlet of the object during the training process;
#X msg 55 581 save_every_x_epochs \$1 \$2 [\$3];
#X text 254 577 Save a model every x epochs;
#X text 262 596 \$1: number of epochs to pass so a model can be saved;
#X text 262 616 \$2: if two arguments are provided \, \$2 is the name of the model to be saved. if three arguments are provided \, \$2 is the first epoch where saving should start;
#X text 261 669 \$3: if three arguments are provided \, \$3 is the name of the model to be saved;
#X text 261 703 An incrementing integer will be concatenated to the name of the model \, for every model that is saved;
#X restore 523 25 pd various_netwrok_messages;
#X text 35 818 4: batch steps count (outputs during training in batches);
#X text 35 859 6: loss (outputs during training and validating);
#X text 35 839 5: accuracy (outputs during training and validating);
#X text 35 917 Check the examples in the "examples" directory for more information on how to create \, train \, test \, and use various kinds of neural networks. Note that 02-mouse_input.pd uses [cyclone/mousestate] and 03-fashion_mnist.pd uses [command] and some Python scripts.;
#X obj 37 1016 neuralnet;
#X text 37 16 [neuralnet] is a feed forward artificial neural network object that supports various different kinds of neural networks \, for classification \, regression \, and binary logistic regreassion. It is written in pure C and does not have any dependencies. It is based on the Python code from the book "Neural Networks from Scratch in Python".;
#X text 209 779 (see [pd various_network_messages] subpatch for details);
#X text 35 779 2: not confident predictions;
#X text 75 203 \$1: number of neurons in input layer;
#X text 75 219 \$2: number of neurons in first hidden layer (N);
#X text 75 235 \$3: if this is the last argument it is the nubmer of neurons in the output layer \, otherwise it is the number of neurons in N+1 hidden layer;
#X text 75 285 ...;
#X text 76 368 E.g. 2 64 64 3 will create a network with two inputs \, two hidden layers with 64 neurons each \, and 3 output neurons;
#X text 75 313 $<N+num_previous_layers>: number of neurons in the output layer (unless there are only three arguments \, in any case \, the last argument is sets the output layer);
#X text 77 429 If you want to load a saved model \, you can write its path as an argument without . For example \, ./examples/models/audio_autoencoder will load the audio_autoencoder.ann model found in the models folder;
#X text 55 183 float args:;
#X text 55 409 symbol args:;
#X text 79 499 To specify that this is the encoder part of an autoencoder \, you have to use the "encoder" argument. The same applies for a decoder with the "decoder" argument. This works only for saved models;
#X text 79 559 If you want to load a specific part of the network \, you can use the "-first_layer" and "-num_layers" arguments. For example \, with a network with seven layers in total \, [neuralnet /path/to/model -first_layer 0 -num_layers 4] will load the first four layers \, where are [neuralnet /path/to/model -first layer 3 -num_layers 4] will load 4 layers \, starting from the third hidden layer (see example 06-audio_autoencdoer.tilde.pd);
